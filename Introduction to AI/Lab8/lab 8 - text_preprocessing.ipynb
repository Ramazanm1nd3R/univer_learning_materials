{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"12\">Sentenize</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_en = \"Hello! How are you? I hope you're doing well. Have a great day.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ru = '''\n",
    "... - \"–¢–∞–∫ –≤ —á–µ–º –∂–µ –¥–µ–ª–æ?\" - \"–ù–µ —Ä–∞-–¥—É-—é—Ç\".\n",
    "... –ò —Ç. –¥. –∏ —Ç. –ø. –í –æ–±—â–µ–º, –≤—Å—è –≥–∞–∑–µ—Ç–∞\n",
    "... '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sabdenoa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def nltk_sentenize(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello!', 'How are you?', \"I hope you're doing well.\", 'Have a great day.']\n",
      "['\\n- \"–¢–∞–∫ –≤ —á–µ–º –∂–µ –¥–µ–ª–æ?\"', '- \"–ù–µ —Ä–∞-–¥—É-—é—Ç\".', '–ò —Ç. –¥. –∏ —Ç. –ø. –í –æ–±—â–µ–º, –≤—Å—è –≥–∞–∑–µ—Ç–∞']\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk_sentenize(text_en)\n",
    "print(sentences)\n",
    "sentences = nltk_sentenize(text_ru)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Substring(0, 6, 'Hello!'),\n",
       " Substring(7, 19, 'How are you?'),\n",
       " Substring(20, 45, \"I hope you're doing well.\"),\n",
       " Substring(46, 63, 'Have a great day.')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from razdel import sentenize\n",
    "sentences =list(sentenize(text_en))\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Substring(1, 23, '- \"–¢–∞–∫ –≤ —á–µ–º –∂–µ –¥–µ–ª–æ?\"'),\n",
       " Substring(24, 40, '- \"–ù–µ —Ä–∞-–¥—É-—é—Ç\".'),\n",
       " Substring(41, 56, '–ò —Ç. –¥. –∏ —Ç. –ø.'),\n",
       " Substring(57, 76, '–í –æ–±—â–µ–º, –≤—Å—è –≥–∞–∑–µ—Ç–∞')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences =list(sentenize(text_ru))\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"12\">Tokenizer</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '–ö—Ä—É–∂–∫–∞-—Ç–µ—Ä–º–æ—Å –Ω–∞ 0.5–ª (50/64 —Å–º¬≥, 516;...)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['–ö—Ä—É–∂–∫–∞-—Ç–µ—Ä–º–æ—Å', '–Ω–∞', '0.5–ª', '(50/64', '—Å–º¬≥,', '516;...)']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = text.split()\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['–ö—Ä—É–∂–∫–∞-—Ç–µ—Ä–º–æ—Å',\n",
       " '–Ω–∞',\n",
       " '0.5–ª',\n",
       " '(',\n",
       " '50/64',\n",
       " '—Å–º¬≥',\n",
       " ',',\n",
       " '516',\n",
       " ';',\n",
       " '...',\n",
       " ')']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nltk_tokenize(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    return words\n",
    "nltk_tokenize(text)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Substring(0, 13, '–ö—Ä—É–∂–∫–∞-—Ç–µ—Ä–º–æ—Å'),\n",
       " Substring(14, 16, '–Ω–∞'),\n",
       " Substring(17, 20, '0.5'),\n",
       " Substring(20, 21, '–ª'),\n",
       " Substring(22, 23, '('),\n",
       " Substring(23, 28, '50/64'),\n",
       " Substring(29, 32, '—Å–º¬≥'),\n",
       " Substring(32, 33, ','),\n",
       " Substring(34, 37, '516'),\n",
       " Substring(37, 38, ';'),\n",
       " Substring(38, 41, '...'),\n",
       " Substring(41, 42, ')')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from razdel import tokenize\n",
    "tokens = list(tokenize(text))\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"12\">Convert  emoticons to text</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             text\n",
      "0  Hello Happy How are you? laugh\n",
      "1            I'm feeling down Sad\n",
      "2         That's hilarious! laugh\n",
      "3        I can't believe it laugh\n",
      "4            This is so sad laugh\n",
      "5            Wink and smile Happy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    \"text\": [\n",
    "        \"Hello :) How are you? :D\",\n",
    "        \"I'm feeling down :(\",\n",
    "        \"That's hilarious! :D\",\n",
    "        \"I can't believe it laugh-)\",\n",
    "        \"This is so sad laugh-(\",\n",
    "        \";) and smile :)\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define the replacements\n",
    "replacements = [\n",
    "    (r\"\\:\\)\", \"Happy\"),\n",
    "    (r\"\\:\\-\\)\", \"Happy\"),\n",
    "    (r\"\\:\\-\\}\", \"Happy\"),\n",
    "    (r\"\\;\\-\\}\", \"Happy\"),\n",
    "    (r\"\\:\\-\\>\", \"Happy\"),\n",
    "    (r\"\\;\\-\\)\", \"Happy\"),\n",
    "    (r\"\\;\\)\", \"Wink\"),\n",
    "    (r\"\\:\\-\\(\", \"Sad\"),\n",
    "    (r\"\\:\\(\", \"Sad\"),\n",
    "    (r\"\\:\\-\\|\", \"Sad\"),\n",
    "    (r\"\\;\\-\\(\", \"Sad\"),\n",
    "    (r\"\\;\\-\\<\", \"Sad\"),\n",
    "    (r\"\\|\\-\\{\", \"Sad\"),\n",
    "    (r\"\\:\\D\", \"laugh\"),\n",
    "    (r\"\\:\\'\\-\\)\", \"tear of joy\"),\n",
    "    (r\"\\:\\`\\-\\(\", \"tear of sadness\"),\n",
    "    (r\"laugh-\\)\", \"laugh\"),\n",
    "    (r\"laugh-\\(\", \"laugh\")\n",
    "]\n",
    "\n",
    "# Apply the replacements\n",
    "for pattern, replacement in replacements:\n",
    "    df[\"text\"] = df[\"text\"].str.replace(pattern, replacement, regex=True)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font size=\"12\">Removing punctuation </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world! Hows  everything going\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# Define the set of punctuation to exclude\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "# Define the set of punctuation to retain\n",
    "not_exclude = {\".\", \"!\", \",\"}\n",
    "\n",
    "# Determine the final set of punctuation to remove\n",
    "final = exclude - not_exclude\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return ''.join(char for char in text if char not in final)\n",
    "\n",
    "# Example usage\n",
    "text = \"Hello, world! How's ()& everything going?\"\n",
    "cleaned_text = remove_punctuation(text)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"12\">Removing stop words</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sabdenoa/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words_en = list(get_stop_words('en'))        \n",
    "nltk_words_en = list(stopwords.words('english')) \n",
    "stop_words_ru = list(get_stop_words('ru'))         \n",
    "nltk_words_ru = list(stopwords.words('russian')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop_words_en = set(get_stop_words('en')) | set(stopwords.words('english'))\n",
    "#stop_words_ru = set(get_stop_words('ru')) | set(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_en=stop_words_en\n",
    "stop_words_ru=stop_words_ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example sentence demonstrate removal stopwords.\n",
      "–ø—Ä–∏–º–µ—Ä –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —É–¥–∞–ª–µ–Ω–∏—è —Å—Ç–æ–ø-—Å–ª–æ–≤.\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords(text, language='en'):\n",
    "    if language == 'en':\n",
    "        stop_words = stop_words_en\n",
    "    elif language == 'ru':\n",
    "        stop_words = stop_words_ru\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported language. Use 'en' for English or 'ru' for Russian.\")\n",
    "    \n",
    "    return ' '.join(word for word in text.split() if word.lower() not in stop_words)\n",
    "\n",
    "# Example usage\n",
    "english_text = \"This is an example sentence to demonstrate the removal of stopwords.\"\n",
    "russian_text = \"–≠—Ç–æ –ø—Ä–∏–º–µ—Ä –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —É–¥–∞–ª–µ–Ω–∏—è —Å—Ç–æ–ø-—Å–ª–æ–≤.\"\n",
    "\n",
    "cleaned_english_text = remove_stopwords(english_text, 'en')\n",
    "print(cleaned_english_text)\n",
    "cleaned_russian_text = remove_stopwords(russian_text, 'ru')\n",
    "print(cleaned_russian_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"12\">Lemmatizer and Stemmer</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.set_proxy('http://proxy-ws.cbank.kz:8080')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/sabdenoa/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/sabdenoa/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kites ---> kite\n",
      "babies ---> baby\n",
      "dogs ---> dog\n",
      "flying ---> flying\n",
      "smiling ---> smiling\n",
      "driving ---> driving\n",
      "died ---> died\n",
      "tried ---> tried\n",
      "feet ---> foot\n",
      "running ---> running\n",
      "are ---> are\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/sabdenoa/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    " \n",
    "# Create WordNetLemmatizer object\n",
    "wnl = WordNetLemmatizer()\n",
    " \n",
    "# single word lemmatization examples\n",
    "list1 = ['kites', 'babies', 'dogs', 'flying', 'smiling', \n",
    "         'driving', 'died', 'tried', 'feet', 'running', 'are']\n",
    "for words in list1:\n",
    "    print(words + \" ---> \" + wnl.lemmatize(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"running\")\n",
    "doc[0].lemma_\n",
    "# for token in doc:\n",
    "#     print(token.text, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmer.stem(\"running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: running | Stem: run | Lemma: run\n",
      "Word: runs | Stem: run | Lemma: run\n",
      "Word: runner | Stem: runner | Lemma: runner\n",
      "Word: easily | Stem: easili | Lemma: easily\n",
      "Word: fairly | Stem: fairli | Lemma: fairly\n",
      "Word: cats | Stem: cat | Lemma: cat\n",
      "Word: studies | Stem: studi | Lemma: study\n",
      "Word: studying | Stem: studi | Lemma: study\n",
      "Word: better | Stem: better | Lemma: better\n",
      "Word: was | Stem: wa | Lemma: be\n",
      "Word: are | Stem: are | Lemma: be\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "\n",
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define a list of words to demonstrate the difference\n",
    "words = [\"running\", \"runs\", \"runner\", \"easily\", \"fairly\", \"cats\", \"studies\", \"studying\", \"better\", \"was\", \"are\"]\n",
    "\n",
    "# Apply stemming and lemmatization\n",
    "stems = [stemmer.stem(word) for word in words]\n",
    "lemmas = [lemmatizer.lemmatize(word, pos='v') for word in words]  # Using 'v' for verb to get accurate results for verbs\n",
    "\n",
    "# Display the results\n",
    "for word, stem, lemma in zip(words, stems, lemmas):\n",
    "    print(f\"Word: {word} | Stem: {stem} | Lemma: {lemma}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: –ö—Ä–∞—Å–∏–≤–∞—è | Lemma: –∫—Ä–∞—Å–∏–≤—ã–π | Stem: –∫—Ä–∞—Å–∏–≤\n",
      "Word: –±–µ–≥–∞—é—â–∞—è | Lemma: –±–µ–≥–∞—Ç—å | Stem: –±–µ–≥–∞\n",
      "Word: –º–∞–º–∞ | Lemma: –º–∞–º–∞ | Stem: –º–∞–º\n",
      "Word: –∫—Ä–∞—Å–∏–≤–æ | Lemma: –∫—Ä–∞—Å–∏–≤–æ | Stem: –∫—Ä–∞—Å–∏–≤\n",
      "Word: –º—ã–ª–∞ | Lemma: –º—ã—Ç—å | Stem: –º—ã–ª\n",
      "Word: —Ä–∞–º—É | Lemma: —Ä–∞–º–∞ | Stem: —Ä–∞–º\n",
      "Word: —É–∑–Ω–∞–≤ | Lemma: —É–∑–Ω–∞–≤–∞—Ç—å | Stem: —É–∑–Ω–∞\n",
      "Word: –ø—Ä–∞–≤–¥—É | Lemma: –ø—Ä–∞–≤–¥–∞ | Stem: –ø—Ä–∞–≤–¥\n"
     ]
    }
   ],
   "source": [
    "from pymystem3 import Mystem\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import nltk\n",
    "\n",
    "# Initialize the Mystem and SnowballStemmer\n",
    "mystem = Mystem()\n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "\n",
    "# Input text\n",
    "text = \"–ö—Ä–∞—Å–∏–≤–∞—è –±–µ–≥–∞—é—â–∞—è –º–∞–º–∞ –∫—Ä–∞—Å–∏–≤–æ –º—ã–ª–∞ —Ä–∞–º—É —É–∑–Ω–∞–≤ –ø—Ä–∞–≤–¥—É\"\n",
    "\n",
    "# Perform lemmatization\n",
    "lemmas = mystem.lemmatize(text)\n",
    "lemmas = [lemma.strip() for lemma in lemmas if lemma.strip()]\n",
    "\n",
    "# Perform stemming\n",
    "words = text.split()\n",
    "stems = [stemmer.stem(word) for word in words]\n",
    "\n",
    "# Display the results\n",
    "for word, lemma, stem in zip(words, lemmas, stems):\n",
    "    print(f\"Word: {word} | Lemma: {lemma} | Stem: {stem}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font size=\"12\">Home tasks</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">1. This task involves normalizing and cleaning a text where some words are replaced with emoticons or the text is split in an unusual way </font>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "–í—á–µ—Ä–∞-–±—ã–ª –ø—Ä–æ—Å—Ç–æ —Å—É–ø–µ—Ä üòä! –ü—Ä–æ–µ—Ö–∞–ª 100/160 –∫–º, —á—Ç–æ–±—ã –≤—Å—Ç—Ä–µ-—Ç–∏—Ç—å—Å—è —Å–æ —Å–≤–æ–∏–º –¥—Ä—É–≥–æ–º. –≠–≥–æ –∏–º—è–ê—Å—Ö–∞—Ç –ê.–ù. –µ–º—É24–≥.–≠—Ç–æ –±—ã–ª–æ –Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–æ –∫—Ä—É—Ç–æ!–û–±—Å—É–∂–¥–∞–ª–∏ –≤—Å–µ, –æ—Ç –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è-–¥–æ –ø–æ–ª–∏—Ç–∏–∫–∏, 2-3—á–∞—Å–∞ –∏–ª–∏–≤–µ—á–Ω–æ—Å—Ç—å –∑–∞ –æ–±–µ–¥–æ–º –≤ —Å—É—à–∏-–±–∞—Ä–µ, –•–ê–•–ê üòÇ. –ü–æ—Ç–æ–º –ø–æ—Å–º–æ—Ç—Ä–µ–ª–∏ —Ñ–∏–ª—å–º–§–û—Ä–µ—Å—Ç –ì.–≤ –∫–∏–Ω–æ –∏ –∑–∞–≥–æ–≤–æ—Ä–∏–ª–∏ –æ —Ç—Ä–µ–Ω–¥–∞—Ö –Ω–∞ —Ä—ã–Ω–∫–µ –∏ —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏—Ö –≤–ª–∏—è–Ω–∏—è—Ö.–ü–æ–≥–æ–¥–∞? –ù–∞—á–∞–ª–æ—Å—å –≤—Å–µ —Å  üåû, –Ω–æ –≤–¥—Ä—É–≥ –ø–æ—à–µ–ª  üåßÔ∏è. –ü–µ—Ä–µ–¥ –æ—Ç—ä–µ–∑–¥–æ–º –∑–∞—à–µ–ª –≤ —É—é—Ç–Ω–æ–µ –∫–∞—Ñ–µ –∑–∞ —á–∞—à–∫–æ–π ‚òï, –æ—á–µ–Ω—å —É—é—Ç–Ω–æ.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">2. Simple Token-Based Search Using Lemmatization and Stemming. This task involves creating a simple token-based search using lemmatization and stemming techniques. Below is a Python function template that takes a user's input and returns the most relevant sentence from a set of 50 sentences.</font>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "banking_sentences = [\n",
    "    \"The bank offers a variety of savings accounts with different interest rates to suit individual needs.\",\n",
    "    \"Customers can easily transfer money between their accounts using the online banking platform.\",\n",
    "    \"Every month, the bank sends out statements detailing all transactions made during the billing cycle.\",\n",
    "    \"Opening a new account at the bank requires a valid ID and proof of address.\",\n",
    "    \"The bank's mobile app allows users to deposit checks by simply taking a photo with their phone.\",\n",
    "    \"Loans for purchasing homes are available with fixed or variable interest rates.\",\n",
    "    \"Credit cards from the bank come with various rewards programs, including cashback and travel points.\",\n",
    "    \"Bank employees are available to help customers understand their financial statements and plan their budgets.\",\n",
    "    \"Savings accounts can be linked to checking accounts to prevent overdraft fees.\",\n",
    "    \"The bank offers financial advice services to help customers plan for retirement.\",\n",
    "    \"Automatic bill payment services help customers avoid missing due dates for important bills.\",\n",
    "    \"Bank branches provide secure safety deposit boxes for storing valuable items.\",\n",
    "    \"Customers can apply for personal loans to cover unexpected expenses or consolidate debt.\",\n",
    "    \"Online banking allows users to set up alerts for low balances or large transactions.\",\n",
    "    \"The bank provides educational resources to help customers understand how to manage their money.\",\n",
    "    \"Customers can choose from a range of investment options, including stocks and bonds.\",\n",
    "    \"The bank has a dedicated customer service line to assist with any account-related issues.\",\n",
    "    \"Mortgage specialists are available to help first-time homebuyers navigate the process.\",\n",
    "    \"The bank's credit monitoring service alerts customers to any changes in their credit reports.\",\n",
    "    \"Customers can easily update their contact information through the bank's online portal.\",\n",
    "    \"The bank's ATM network provides convenient access to cash withdrawals and deposits.\",\n",
    "    \"Foreign currency exchange services are available for customers planning international travel.\",\n",
    "    \"The bank's fraud protection service monitors accounts for suspicious activity.\",\n",
    "    \"Customers can set spending limits on their credit cards to help manage their budgets.\",\n",
    "    \"The bank offers a range of insurance products, including health, auto, and home insurance.\",\n",
    "    \"Users can download and print monthly account statements directly from the bank's website.\",\n",
    "    \"Small business owners can access loans and lines of credit to help grow their businesses.\",\n",
    "    \"The bank provides secure online payment options for shopping on various e-commerce platforms.\",\n",
    "    \"Mobile banking apps allow users to check their account balances on the go.\",\n",
    "    \"The bank's retirement accounts offer tax advantages to help customers save for the future.\",\n",
    "    \"Financial advisors at the bank can help customers develop long-term investment strategies.\",\n",
    "    \"The bank offers low-interest loans for education and other major life expenses.\",\n",
    "    \"Customers can make international money transfers at competitive exchange rates.\",\n",
    "    \"The bank's secure online platform protects customers' personal and financial information.\",\n",
    "    \"Home equity lines of credit are available for homeowners needing access to cash.\",\n",
    "    \"The bank offers fixed-term deposits with higher interest rates for long-term savings.\",\n",
    "    \"Customers can access their account information 24/7 through the bank's mobile app.\",\n",
    "    \"The bank's customer loyalty programs offer benefits such as reduced fees and higher interest rates.\",\n",
    "    \"Parents can open savings accounts for their children to teach them about money management.\",\n",
    "    \"The bank's budgeting tools help customers track their spending and set financial goals.\",\n",
    "    \"Customers can sign up for direct deposit to have their paychecks automatically deposited.\",\n",
    "    \"The bank offers special accounts for students with no monthly fees and low minimum balances.\",\n",
    "    \"Users can schedule future payments and transfers using the bank's online banking services.\",\n",
    "    \"The bank's investment products include mutual funds and retirement accounts.\",\n",
    "    \"Customers can access financial planning services to help manage their assets and liabilities.\",\n",
    "    \"The bank provides loan calculators to help customers estimate their monthly payments.\",\n",
    "    \"Secure messaging through the bank's online platform allows customers to communicate with their advisors.\",\n",
    "    \"The bank's mobile check deposit feature makes it easy to deposit checks without visiting a branch.\",\n",
    "    \"Customers can earn rewards points for every dollar spent on their credit cards.\",\n",
    "    \"The bank offers personal finance workshops to educate customers on budgeting and saving.\",\n",
    "    \"Customers can track their spending habits using the bank's online expense tracking tools.\",\n",
    "    \"–ë–∞–Ω–∫ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ —Å–±–µ—Ä–µ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Å—á–µ—Ç–∞ —Å —Ä–∞–∑–Ω—ã–º–∏ –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã–º–∏ —Å—Ç–∞–≤–∫–∞–º–∏, —á—Ç–æ–±—ã —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç—å –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–∏.\",\n",
    "    \"–ö–ª–∏–µ–Ω—Ç—ã –º–æ–≥—É—Ç –ª–µ–≥–∫–æ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å –¥–µ–Ω—å–≥–∏ –º–µ–∂–¥—É —Å–≤–æ–∏–º–∏ —Å—á–µ—Ç–∞–º–∏ —Å –ø–æ–º–æ—â—å—é –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–±–∞–Ω–∫–∏–Ω–≥–∞.\",\n",
    "    \"–ö–∞–∂–¥—ã–π –º–µ—Å—è—Ü –±–∞–Ω–∫ –≤—ã—Å—ã–ª–∞–µ—Ç –≤—ã–ø–∏—Å–∫–∏ —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º –æ–ø–∏—Å–∞–Ω–∏–µ–º –≤—Å–µ—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –∑–∞ –æ—Ç—á–µ—Ç–Ω—ã–π –ø–µ—Ä–∏–æ–¥.\",\n",
    "    \"–û—Ç–∫—Ä—ã—Ç–∏–µ –Ω–æ–≤–æ–≥–æ —Å—á–µ—Ç–∞ –≤ –±–∞–Ω–∫–µ —Ç—Ä–µ–±—É–µ—Ç –¥–µ–π—Å—Ç–≤—É—é—â–µ–≥–æ —É–¥–æ—Å—Ç–æ–≤–µ—Ä–µ–Ω–∏—è –ª–∏—á–Ω–æ—Å—Ç–∏ –∏ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –∞–¥—Ä–µ—Å–∞.\",\n",
    "    \"–ú–æ–±–∏–ª—å–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –±–∞–Ω–∫–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º –≤–Ω–æ—Å–∏—Ç—å —á–µ–∫–∏, –ø—Ä–æ—Å—Ç–æ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—Ä—É—è –∏—Ö –Ω–∞ —Ç–µ–ª–µ—Ñ–æ–Ω.\",\n",
    "    \"–ö—Ä–µ–¥–∏—Ç—ã –Ω–∞ –ø–æ–∫—É–ø–∫—É –¥–æ–º–æ–≤ –¥–æ—Å—Ç—É–ø–Ω—ã —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∏–ª–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã–º–∏ —Å—Ç–∞–≤–∫–∞–º–∏.\",\n",
    "    \"–ö—Ä–µ–¥–∏—Ç–Ω—ã–µ –∫–∞—Ä—Ç—ã –±–∞–Ω–∫–∞ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π, –≤–∫–ª—é—á–∞—è –∫—ç—à–±—ç–∫ –∏ –±–æ–Ω—É—Å–Ω—ã–µ –±–∞–ª–ª—ã –¥–ª—è –ø—É—Ç–µ—à–µ—Å—Ç–≤–∏–π.\",\n",
    "    \"–°–æ—Ç—Ä—É–¥–Ω–∏–∫–∏ –±–∞–Ω–∫–∞ –≥–æ—Ç–æ–≤—ã –ø–æ–º–æ—á—å –∫–ª–∏–µ–Ω—Ç–∞–º –ø–æ–Ω—è—Ç—å —Å–≤–æ–∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –≤—ã–ø–∏—Å–∫–∏ –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –±—é–¥–∂–µ—Ç.\",\n",
    "    \"–°–±–µ—Ä–µ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Å—á–µ—Ç–∞ –º–æ–≥—É—Ç –±—ã—Ç—å —Å–≤—è–∑–∞–Ω—ã —Å —Ç–µ–∫—É—â–∏–º–∏ —Å—á–µ—Ç–∞–º–∏, —á—Ç–æ–±—ã –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç—å –∫–æ–º–∏—Å—Å–∏–∏ –∑–∞ –ø–µ—Ä–µ—Ä–∞—Å—Ö–æ–¥.\",\n",
    "    \"–ë–∞–Ω–∫ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —É—Å–ª—É–≥–∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ –∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, —á—Ç–æ–±—ã –ø–æ–º–æ—á—å –∫–ª–∏–µ–Ω—Ç–∞–º –ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –≤—ã—Ö–æ–¥ –Ω–∞ –ø–µ–Ω—Å–∏—é.\",\n",
    "    \"–£—Å–ª—É–≥–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ–ø–ª–∞—Ç—ã —Å—á–µ—Ç–æ–≤ –ø–æ–º–æ–≥–∞—é—Ç –∫–ª–∏–µ–Ω—Ç–∞–º –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–æ–ø—É—Å–∫–∞ —Å—Ä–æ–∫–æ–≤ –æ–ø–ª–∞—Ç—ã –≤–∞–∂–Ω—ã—Ö —Å—á–µ—Ç–æ–≤.\",\n",
    "    \"–û—Ç–¥–µ–ª–µ–Ω–∏—è –±–∞–Ω–∫–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ —è—á–µ–π–∫–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ü–µ–Ω–Ω—ã—Ö –≤–µ—â–µ–π.\",\n",
    "    \"–ö–ª–∏–µ–Ω—Ç—ã –º–æ–≥—É—Ç –ø–æ–¥–∞—Ç—å –∑–∞—è–≤–∫—É –Ω–∞ –ª–∏—á–Ω—ã–µ –∫—Ä–µ–¥–∏—Ç—ã –¥–ª—è –ø–æ–∫—Ä—ã—Ç–∏—è –Ω–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω—ã—Ö —Ä–∞—Å—Ö–æ–¥–æ–≤ –∏–ª–∏ –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏ –¥–æ–ª–≥–æ–≤.\",\n",
    "    \"–ò–Ω—Ç–µ—Ä–Ω–µ—Ç-–±–∞–Ω–∫–∏–Ω–≥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –æ –Ω–∏–∑–∫–∏—Ö –æ—Å—Ç–∞—Ç–∫–∞—Ö –∏–ª–∏ –∫—Ä—É–ø–Ω—ã—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è—Ö.\",\n",
    "    \"–ë–∞–Ω–∫ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã, —á—Ç–æ–±—ã –ø–æ–º–æ—á—å –∫–ª–∏–µ–Ω—Ç–∞–º –ø–æ–Ω—è—Ç—å, –∫–∞–∫ —É–ø—Ä–∞–≤–ª—è—Ç—å —Å–≤–æ–∏–º–∏ –¥–µ–Ω—å–≥–∞–º–∏.\",\n",
    "    \"–ö–ª–∏–µ–Ω—Ç—ã –º–æ–≥—É—Ç –≤—ã–±–∏—Ä–∞—Ç—å –∏–∑ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã—Ö –æ–ø—Ü–∏–π, –≤–∫–ª—é—á–∞—è –∞–∫—Ü–∏–∏ –∏ –æ–±–ª–∏–≥–∞—Ü–∏–∏.\",\n",
    "    \"–£ –±–∞–Ω–∫–∞ –µ—Å—Ç—å –≤—ã–¥–µ–ª–µ–Ω–Ω–∞—è –ª–∏–Ω–∏—è –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–æ–º–æ—â–∏ –≤ –ª—é–±—ã—Ö –≤–æ–ø—Ä–æ—Å–∞—Ö, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å–æ —Å—á–µ—Ç–∞–º–∏.\",\n",
    "    \"–°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—ã –ø–æ –∏–ø–æ—Ç–µ–∫–µ –≥–æ—Ç–æ–≤—ã –ø–æ–º–æ—á—å –ø–æ–∫—É–ø–∞—Ç–µ–ª—è–º –≤–ø–µ—Ä–≤—ã–µ –∫—É–ø–∏—Ç—å –∂–∏–ª—å–µ –ø—Ä–æ–π—Ç–∏ —ç—Ç–æ—Ç –ø—Ä–æ—Ü–µ—Å—Å.\",\n",
    "    \"–°–ª—É–∂–±–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∫—Ä–µ–¥–∏—Ç–æ–≤ –±–∞–Ω–∫–∞ —É–≤–µ–¥–æ–º–ª—è–µ—Ç –∫–ª–∏–µ–Ω—Ç–æ–≤ –æ –ª—é–±—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏—è—Ö –≤ –∏—Ö –∫—Ä–µ–¥–∏—Ç–Ω—ã—Ö –æ—Ç—á–µ—Ç–∞—Ö.\",\n",
    "    \"–ö–ª–∏–µ–Ω—Ç—ã –º–æ–≥—É—Ç –ª–µ–≥–∫–æ –æ–±–Ω–æ–≤–∏—Ç—å —Å–≤–æ—é –∫–æ–Ω—Ç–∞–∫—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —á–µ—Ä–µ–∑ –æ–Ω–ª–∞–π–Ω-–ø–æ—Ä—Ç–∞–ª –±–∞–Ω–∫–∞.\",\n",
    "    \"–°–µ—Ç—å –±–∞–Ω–∫–æ–º–∞—Ç–æ–≤ –±–∞–Ω–∫–∞ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —É–¥–æ–±–Ω—ã–π –¥–æ—Å—Ç—É–ø –∫ —Å–Ω—è—Ç–∏—é –∏ –≤–Ω–µ—Å–µ–Ω–∏—é –Ω–∞–ª–∏—á–Ω—ã—Ö.\",\n",
    "    \"–£—Å–ª—É–≥–∏ –æ–±–º–µ–Ω–∞ –∏–Ω–æ—Å—Ç—Ä–∞–Ω–Ω–æ–π –≤–∞–ª—é—Ç—ã –¥–æ—Å—Ç—É–ø–Ω—ã –¥–ª—è –∫–ª–∏–µ–Ω—Ç–æ–≤, –ø–ª–∞–Ω–∏—Ä—É—é—â–∏—Ö –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –ø–æ–µ–∑–¥–∫–∏.\",\n",
    "    \"–°–ª—É–∂–±–∞ –∑–∞—â–∏—Ç—ã –æ—Ç –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞ –±–∞–Ω–∫–∞ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç —Å—á–µ—Ç–∞ –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏.\",\n",
    "    \"–ö–ª–∏–µ–Ω—Ç—ã –º–æ–≥—É—Ç —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å –ª–∏–º–∏—Ç—ã —Ä–∞—Å—Ö–æ–¥–æ–≤ –ø–æ —Å–≤–æ–∏–º –∫—Ä–µ–¥–∏—Ç–Ω—ã–º –∫–∞—Ä—Ç–∞–º, —á—Ç–æ–±—ã –ø–æ–º–æ—á—å —É–ø—Ä–∞–≤–ª—è—Ç—å –±—é–¥–∂–µ—Ç–æ–º.\",\n",
    "    \"–ë–∞–Ω–∫ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ —Å—Ç—Ä–∞—Ö–æ–≤—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã, –≤–∫–ª—é—á–∞—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–µ, –∞–≤—Ç–æ–º–æ–±–∏–ª—å–Ω–æ–µ –∏ –∂–∏–ª–∏—â–Ω–æ–µ —Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ.\",\n",
    "    \"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –º–æ–≥—É—Ç –∑–∞–≥—Ä—É–∂–∞—Ç—å –∏ –ø–µ—á–∞—Ç–∞—Ç—å –µ–∂–µ–º–µ—Å—è—á–Ω—ã–µ –≤—ã–ø–∏—Å–∫–∏ –ø–æ —Å—á–µ—Ç–∞–º –ø—Ä—è–º–æ —Å —Å–∞–π—Ç–∞ –±–∞–Ω–∫–∞.\",\n",
    "    \"–ú–∞–ª—ã–µ –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏—è –º–æ–≥—É—Ç –ø–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø –∫ –∫—Ä–µ–¥–∏—Ç–∞–º –∏ –∫—Ä–µ–¥–∏—Ç–Ω—ã–º –ª–∏–Ω–∏—è–º –¥–ª—è —Ä–æ—Å—Ç–∞ —Å–≤–æ–∏—Ö –±–∏–∑–Ω–µ—Å–æ–≤.\",\n",
    "    \"–ë–∞–Ω–∫ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ –æ–Ω–ª–∞–π–Ω-–æ–ø—Ü–∏–∏ –æ–ø–ª–∞—Ç—ã –¥–ª—è –ø–æ–∫—É–ø–æ–∫ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞—Ö —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–π –∫–æ–º–º–µ—Ä—Ü–∏–∏.\",\n",
    "    \"–ú–æ–±–∏–ª—å–Ω—ã–µ –±–∞–Ω–∫–æ–≤—Å–∫–∏–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –ø–æ–∑–≤–æ–ª—è—é—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º –ø—Ä–æ–≤–µ—Ä—è—Ç—å —Å–≤–æ–∏ –±–∞–ª–∞–Ω—Å—ã —Å—á–µ—Ç–æ–≤ –Ω–∞ —Ö–æ–¥—É.\",\n",
    "    \"–ü–µ–Ω—Å–∏–æ–Ω–Ω—ã–µ —Å—á–µ—Ç–∞ –±–∞–Ω–∫–∞ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–∞–ª–æ–≥–æ–≤—ã–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞, —á—Ç–æ–±—ã –ø–æ–º–æ—á—å –∫–ª–∏–µ–Ω—Ç–∞–º —Å—ç–∫–æ–Ω–æ–º–∏—Ç—å –Ω–∞ –±—É–¥—É—â–µ–µ.\",\n",
    "    \"–§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç—ã –±–∞–Ω–∫–∞ –º–æ–≥—É—Ç –ø–æ–º–æ—á—å –∫–ª–∏–µ–Ω—Ç–∞–º —Ä–∞–∑—Ä–∞–±–æ—Ç–∞—Ç—å –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏.\",\n",
    "    \"–ë–∞–Ω–∫ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –∫—Ä–µ–¥–∏—Ç—ã —Å –Ω–∏–∑–∫–∏–º–∏ –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã–º–∏ —Å—Ç–∞–≤–∫–∞–º–∏ –¥–ª—è –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏ –¥—Ä—É–≥–∏—Ö –∫—Ä—É–ø–Ω—ã—Ö –∂–∏–∑–Ω–µ–Ω–Ω—ã—Ö —Ä–∞—Å—Ö–æ–¥–æ–≤.\",\n",
    "    \"–ö–ª–∏–µ–Ω—Ç—ã –º–æ–≥—É—Ç –æ—Å—É—â–µ—Å—Ç–≤–ª—è—Ç—å –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –¥–µ–Ω–µ–∂–Ω—ã–µ –ø–µ—Ä–µ–≤–æ–¥—ã –ø–æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—ã–º –æ–±–º–µ–Ω–Ω—ã–º –∫—É—Ä—Å–∞–º.\",\n",
    "    \"–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ–Ω–ª–∞–π–Ω-–ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –±–∞–Ω–∫–∞ –∑–∞—â–∏—â–∞–µ—Ç –ª–∏—á–Ω—É—é –∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∫–ª–∏–µ–Ω—Ç–æ–≤.\",\n",
    "    \"–ö—Ä–µ–¥–∏—Ç–Ω—ã–µ –ª–∏–Ω–∏–∏ –ø–æ–¥ –∑–∞–ª–æ–≥ –¥–æ–º–∞ –¥–æ—Å—Ç—É–ø–Ω—ã –¥–ª—è –≤–ª–∞–¥–µ–ª—å—Ü–µ–≤ –¥–æ–º–æ–≤, –Ω—É–∂–¥–∞—é—â–∏—Ö—Å—è –≤ –¥–æ—Å—Ç—É–ø–µ –∫ –Ω–∞–ª–∏—á–Ω—ã–º.\",\n",
    "    \"–ë–∞–Ω–∫ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –¥–µ–ø–æ–∑–∏—Ç—ã —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —Å—Ä–æ–∫–æ–º —Å –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–º–∏ –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã–º–∏ —Å—Ç–∞–≤–∫–∞–º–∏ –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö —Å–±–µ—Ä–µ–∂–µ–Ω–∏–π.\",\n",
    "    \"–ö–ª–∏–µ–Ω—Ç—ã –º–æ–≥—É—Ç –ø–æ–ª—É—á–∞—Ç—å –¥–æ—Å—Ç—É–ø –∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ —Å—á–µ—Ç—É –∫—Ä—É–≥–ª–æ—Å—É—Ç–æ—á–Ω–æ —á–µ—Ä–µ–∑ –º–æ–±–∏–ª—å–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –±–∞–Ω–∫–∞.\",\n",
    "    \"–ü—Ä–æ–≥—Ä–∞–º–º—ã –ª–æ—è–ª—å–Ω–æ—Å—Ç–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤ –±–∞–Ω–∫–∞ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ç–∞–∫–∏–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞, –∫–∞–∫ —Å–Ω–∏–∂–µ–Ω–Ω—ã–µ –∫–æ–º–∏—Å—Å–∏–∏ –∏ –ø–æ–≤—ã—à–µ–Ω–Ω—ã–µ –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã–µ —Å—Ç–∞–≤–∫–∏.\",\n",
    "    \"–†–æ–¥–∏—Ç–µ–ª–∏ –º–æ–≥—É—Ç –æ—Ç–∫—Ä—ã—Ç—å —Å–±–µ—Ä–µ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Å—á–µ—Ç–∞ –¥–ª—è —Å–≤–æ–∏—Ö –¥–µ—Ç–µ–π, —á—Ç–æ–±—ã –Ω–∞—É—á–∏—Ç—å –∏—Ö —É–ø—Ä–∞–≤–ª–µ–Ω–∏—é –¥–µ–Ω—å–≥–∞–º–∏.\",\n",
    "    \"–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã —Å–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –±—é–¥–∂–µ—Ç–∞ –±–∞–Ω–∫–∞ –ø–æ–º–æ–≥–∞—é—Ç –∫–ª–∏–µ–Ω—Ç–∞–º –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å —Å–≤–æ–∏ —Ä–∞—Å—Ö–æ–¥—ã –∏ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ —Ü–µ–ª–∏.\",\n",
    "    \"–ö–ª–∏–µ–Ω—Ç—ã –º–æ–≥—É—Ç –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è –¥–ª—è –ø—Ä—è–º–æ–≥–æ –¥–µ–ø–æ–∑–∏—Ç–∞, —á—Ç–æ–±—ã –∏—Ö –∑–∞—Ä–ø–ª–∞—Ç—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞—á–∏—Å–ª—è–ª–∏—Å—å –Ω–∞ —Å—á–µ—Ç.\",\n",
    "    \"–ë–∞–Ω–∫ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å—á–µ—Ç–∞ –¥–ª—è —Å—Ç—É–¥–µ–Ω—Ç–æ–≤ –±–µ–∑ –µ–∂–µ–º–µ—Å—è—á–Ω—ã—Ö –∫–æ–º–∏—Å—Å–∏–π –∏ —Å –Ω–∏–∑–∫–∏–º–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –æ—Å—Ç–∞—Ç–∫–∞–º–∏.\",\n",
    "    \"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –º–æ–≥—É—Ç –ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –±—É–¥—É—â–∏–µ –ø–ª–∞—Ç–µ–∂–∏ –∏ –ø–µ—Ä–µ–≤–æ–¥—ã, –∏—Å–ø–æ–ª—å–∑—É—è —É—Å–ª—É–≥–∏ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–±–∞–Ω–∫–∏–Ω–≥–∞ –±–∞–Ω–∫–∞.\",\n",
    "    \"–ò–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã –±–∞–Ω–∫–∞ –≤–∫–ª—é—á–∞—é—Ç –≤–∑–∞–∏–º–Ω—ã–µ —Ñ–æ–Ω–¥—ã –∏ –ø–µ–Ω—Å–∏–æ–Ω–Ω—ã–µ —Å—á–µ—Ç–∞.\",\n",
    "    \"–ö–ª–∏–µ–Ω—Ç—ã –º–æ–≥—É—Ç –ø–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø –∫ —É—Å–ª—É–≥–∞–º —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Å–≤–æ–∏–º–∏ –∞–∫—Ç–∏–≤–∞–º–∏ –∏ –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞–º–∏.\",\n",
    "    \"–ë–∞–Ω–∫ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∫—Ä–µ–¥–∏—Ç–Ω—ã–µ –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä—ã, —á—Ç–æ–±—ã –ø–æ–º–æ—á—å –∫–ª–∏–µ–Ω—Ç–∞–º –æ—Ü–µ–Ω–∏—Ç—å –∏—Ö –µ–∂–µ–º–µ—Å—è—á–Ω—ã–µ –ø–ª–∞—Ç–µ–∂–∏.\",\n",
    "    \"–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –æ–±–º–µ–Ω —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏ —á–µ—Ä–µ–∑ –æ–Ω–ª–∞–π–Ω-–ø–ª–∞—Ç—Ñ–æ—Ä–º—É –±–∞–Ω–∫–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∫–ª–∏–µ–Ω—Ç–∞–º –æ–±—â–∞—Ç—å—Å—è —Å–æ —Å–≤–æ–∏–º–∏ –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç–∞–º–∏.\",\n",
    "    \"–§—É–Ω–∫—Ü–∏—è –º–æ–±–∏–ª—å–Ω–æ–≥–æ –¥–µ–ø–æ–∑–∏—Ç–∞ —á–µ–∫–æ–≤ –±–∞–Ω–∫–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ª–µ–≥–∫–æ –≤–Ω–æ—Å–∏—Ç—å —á–µ–∫–∏ –±–µ–∑ –ø–æ—Å–µ—â–µ–Ω–∏—è –æ—Ç–¥–µ–ª–µ–Ω–∏—è.\",\n",
    "    \"–ö–ª–∏–µ–Ω—Ç—ã –º–æ–≥—É—Ç –∑–∞—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –±–∞–ª–ª—ã –∑–∞ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ –∑–∞ –∫–∞–∂–¥—ã–π –ø–æ—Ç—Ä–∞—á–µ–Ω–Ω—ã–π –¥–æ–ª–ª–∞—Ä –Ω–∞ —Å–≤–æ–∏—Ö –∫—Ä–µ–¥–∏—Ç–Ω—ã—Ö –∫–∞—Ä—Ç–∞—Ö.\",\n",
    "    \"–ë–∞–Ω–∫ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Å–µ–º–∏–Ω–∞—Ä—ã –ø–æ –ª–∏—á–Ω—ã–º —Ñ–∏–Ω–∞–Ω—Å–∞–º, —á—Ç–æ–±—ã –æ–±—É—á–∏—Ç—å –∫–ª–∏–µ–Ω—Ç–æ–≤ —Å–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—é –±—é–¥–∂–µ—Ç–∞ –∏ —Å–±–µ—Ä–µ–∂–µ–Ω–∏—è–º.\",\n",
    "    \"–ö–ª–∏–µ–Ω—Ç—ã –º–æ–≥—É—Ç –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å —Å–≤–æ–∏ –ø—Ä–∏–≤—ã—á–∫–∏ —Ç—Ä–∞—Ç —Å –ø–æ–º–æ—â—å—é –æ–Ω–ª–∞–π–Ω-–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —Ä–∞—Å—Ö–æ–¥–æ–≤ –±–∞–Ω–∫–∞.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def preprocess(text):\n",
    "    return tokens \n",
    "\n",
    "def search(query):\n",
    "    #score = words query exist in database token set.\n",
    "    return #sentence , score\n",
    "\n",
    "# Example usage\n",
    "user_query = \"Type your text here\"\n",
    "print(search(user_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">3. Text Cleaning by Removing Blacklist Words or Phrases</font>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a sample text including the word *** is *** and *** and *** ***.\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    blacklist = ['nigga', 'fucking', '–≥–µ–π', '—É—Ä–æ–¥', '–ü—É—Ç–∏–Ω', '–¢–æ–∫–∞–µ–≤ –¢–∏–≥—Ä']\n",
    "    for word in blacklist:\n",
    "        text = text.replace(word, '***')\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "sample_text = \"–ü—É—Ç–∏–Ω Here is a sample text including the word is nigga and –≤—Å–µ fucking –∫—Ä–∞—Å–∏–≤—ã–µ –ª—é–¥–∏ and –¢–æ–∫–∞–µ–≤ –¢–∏–≥—Ä.\"\n",
    "print(clean_text(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TEST ENVIRONMENT",
   "language": "python",
   "name": "test_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
